---
layout: default
---

[[Google Scholar](https://scholar.google.com/citations?hl=en&user=5e9tBtQAAAAJ&view_op=list_works&gmla=AJsN-F6ieV5-6P_WzCdbvRYvxWSI33-VELtb0CU6B5dRbXHRE5PhOLn2bmG_5XkhAUdOEgKxiZd864yv2IVcuooJbWq6x7N7lL1nm_vxeK_QPHLncFhdjSA)]&emsp;[[Twitter](https://twitter.com/YangyiChen6666)]&emsp;[[Github](https://github.com/Yangyi-Chen)]



<!-- google-site-verification: google839c7dfb26c92343.html [2023.8]. 1 paper accepted to TACL.

[2023.7]. Receive the outstanding reviewer award for ACL 2023.

[2023.5]. Start my summer internship at SRI International (NJ). [2023.5]. 4 papers accepted to ACL 2023 (3 findings).-->
# News
[2024.11]. 1 paper accepted to TMLR.

[2024.9]. 1 paper accepted to EMNLP 2024.

[2024.6]. Serve as Area Chairs/Action Editors for ACL, EMNLP, ARR.

[2024.5]. 1 paper accepted to ICML 2024.

[2024.3]. 2 papers accepted to NAACL 2024 (1 outstanding paper).

[2024.2]. 1 paper accepted to CVPR 2024.

[2024.1]. 2 papers accepted to ICLR 2024.

[2023]. ......



# Bio
I'm a third-year CS Ph.D. student at UIUC. I'm advised by Prof. [Heng Ji](http://blender.cs.illinois.edu/hengji.html) and I also collaborate with Prof. [Hao Peng](https://haopeng-nlp.github.io/).
I work on **scalable foundation models**, aiming to establish fundamental approaches to address the following challenges:
- **Predictable Scaling**: The scaling laws that can reliably predict the development of foundation models.
- **Scalable Modeling**: The model architectures and training recipes for multimodality that can transform increasing compute and data into sustained performance gains without hitting any bottleneck.
- **Scalable Oversight**: The scalable post-training approaches that can continually improve the foundation models and align their objectives with human intents even as they exceed human capabilities.

  






<!--The long-term goal of my research is to build aligned and interactive AI systems to address challenges that remain unresolved even for human capabilities. To achieve this goal, my current research primarily concentrates on multimodal and large language models, aiming to establish fundamental approaches to address the following challenges:
- **Alignment**: How to train AI systems to follow human intents and values?
- **Interaction**: How to train AI systems to effectively interact with external entities (e.g., tools, humans) in the environment to facilitate the acquisition of information and language feedback?
- **Supervision**: How to train and evaluate AI systems that surpass the capabilities of human counterparts?-->





In my undergraduate years, I worked on **AI safety**. I was a research intern at [THUNLP](https://nlp.csai.tsinghua.edu.cn) advised by Prof. [Zhiyuan Liu](http://nlp.csai.tsinghua.edu.cn/~lzy/). In the early stages of my research, I worked closely with Dr. [Fanchao Qi](https://fanchao-qi.github.io/) and received great help from him. Also, I am delighted to work with Prof. [Wei Wei](https://www.eric-weiwei.com) and Prof. [Dawn Song](https://people.eecs.berkeley.edu/~dawnsong/). 





# Selected Publications 
[[Full Publications](./publications.html)] 

**<sup>\*</sup>  indicates equal contribution <sup>\+</sup>  indicates corresponding author**
- **Scaling Laws for Predicting Downstream Performance in LLMs** [[paper](https://arxiv.org/abs/2410.08527)] <br/> **Yangyi Chen**, Binxuan Huang, Yifan Gao, Zhengyang Wang, Jingfeng Yang, Heng Ji <br/> **Arxiv**

- **A Single Transformer for Scalable Vision-Language Modeling** [[paper](https://arxiv.org/abs/2407.06438)] <br/> **Yangyi Chen<sup>\*</sup>**, Xingyao Wang<sup>\*</sup>, Hao Peng, Heng Ji <br/> **TMLR 2024**

- **SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales** [[paper](https://arxiv.org/abs/2405.20974)] <br/> Tianyang Xu<sup>\*</sup>, Shujin Wu<sup>\*</sup>, Shizhe Diao, Xiaoze Liu, Xingyao Wang, **Yangyi Chen<sup>\+</sup>**, Jing Gao<sup>\+</sup> <br/> **EMNLP 2024**

- **DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback** [[paper](http://arxiv.org/abs/2311.10081)] <br/> **Yangyi Chen**, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran <br/> **CVPR 2024**

- **A Close Look into the Calibration of Pre-trained Language Models** [[paper](https://arxiv.org/abs/2211.00151)] <br/> **Yangyi Chen<sup>\*</sup>**, Lifan Yuan<sup>\*</sup>, Ganqu Cui, Zhiyuan Liu, Heng Ji. <br/> **ACL 2023** 

<!--- **Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models** [[paper](https://arxiv.org/abs/2309.04461)] <br/> **Yangyi Chen**, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran. <br/> **NAACL 2024** 
- **Executable Code Actions Elicit Better LLM Agents** [[paper](https://arxiv.org/abs/2402.01030)] <br/> Xingyao Wang, **Yangyi Chen**, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, Heng Ji <br/> **ICML 2024**
- **A Close Look into the Calibration of Pre-trained Language Models** [[paper](https://arxiv.org/abs/2211.00151)] <br/> **Yangyi Chen<sup>\*</sup>**, Lifan Yuan<sup>\*</sup>, Ganqu Cui, Zhiyuan Liu, Heng Ji. <br/> **ACL 2023** 
- **CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets** [[paper](https://arxiv.org/abs/2309.17428)] <br/> Lifan Yuan<sup>\*</sup>, **Yangyi Chen**<sup>\*</sup>, Xingyao Wang, Yi R. Fung, Hao Peng, Heng Ji. <br/> **ICLR 2024**
-->



<!-- - **Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP** [[paper](https://arxiv.org/abs/2210.10683)] <br/> **Yangyi Chen<sup>\*</sup>**, Hongcheng Gao<sup>\*</sup>, Ganqu Cui, Fanchao Qi, Longtao Huang, Zhiyuan Liu, Maosong Sun.  <br/> **EMNLP 2022** -->





# Education
- University of Illinois Urbana-Champaign, Ph.D. in Computer Science; 2022-Present

- Huazhong University of Science and Technology, BS in Software Engineering;    2018-2022

# Experience
- Microsoft (Remote), Part-Time Research Intern; Feb-May (expected) 2024; Hosted by Dr. [Zhengyuan Yang](https://zyang-ur.github.io/). 

- Amazon (Palo Alto), Applied Scientist Research Intern; May-Aug 2024; Hosted by Dr. [Binxuan Huang](https://binxuan.github.io/), Dr. [Yifan Gao](https://yifan-gao.github.io/), and Dr. [Zhengyang Wang](https://zhengyang-wang.github.io/). Topic: Scaling Laws for LLMs.

- SRI International (New Jersey), Research Intern; May-Aug 2023; Hosted by Dr. [Karan Sikka](https://www.ksikka.com/), Dr. [Michael Cogswell](http://mcogswell.io/), and Dr. [Ajay Divakaran](https://www.sri.com/bios/ajay-divakaran/). Topic: Scalable Supervision for Large Vision-Language Models. 

# Service
## Area Chair/Action Editor
**2025**: ACL/ARR (Feb)

**2024**: ACL/ARR (Feb), ARR (Apr, Aug, Oct), EMNLP/ARR (Jun)

## Reviewer
**2025**: COLM, ICML 

**2024**: COLM, ICML, ICLR

**2023**: ICLR, EMNLP, EMNLP Industry Track, NeurIPS, NeurIPS D&B Track, ARR (Feb, Apr, Jun, Aug, Oct, Dec), ACL (**Outstanding Award**), IEEE T-IFS 

**2022**: NeurIPS D&B Track, EMNLP, ARR (Dec)

**Assistant**: AAAI 2022, ARR 2022 (Jan), EMNLP 2021, ARR 2021 (Oct, Nov) 
<!-- **Reviewer**: NeurIPS 2023, ARR 2023 (Feb, Apr), ACL 2023, IEEE T-IFS, NeurIPS 2022, EMNLP 2022, ARR 2022 (Dec) My research goal is to develop general-purpose models that can follow human instructions to solve tasks in a zero- or few-shot manner. I identify two important directions towards this goal and focus my research on (1) How to effectively acquire knowledge from web-scale data? (2) How to elicit the knowledge stored in pre-trained models to perform downstream tasks? 2 papers accepted to EMNLP 2022.[2022.9]. 2 papers accepted to NeurIPS 2022 (1 Spotlight).[2022.8]. Start my PhD journey at UIUC!![2022.4]. 1 paper accepted to NAACL 2022 (findings).[2021.8]. 3 papers accepted to EMNLP 2021. [2021.5]. 2 papers accepted to ACL 2021 (1 findings).I work on multimodal and large language models, basically focusing on three high-level topics: - **Fundamental development**: How to effectively develop general-purpose pre-trained models that possess strong fundamental capabilities (e.g., reasoning)?- **Exploring and exploiting their extensive potential**: How to fully leverage the capabilities of models to address real-world challenges?- **In-depth analysis**: How to systematically evaluate and interpret the behaviors of models? -->



# Personal
When I was young (around 20), I enjoyed playing the piano and **basketball**. When I started doing research, my interests changed and I quickly fell in love with all kinds of Chinese food and **mobile basketball games**. But always, I am a big fan of milk tea!! 


<!-- I enjoy playing the piano and basketball in my free time. Recently, I've been enjoying suspense/thriller movies. Finally, I've always been a big fan of milk tea!!! -->


 
